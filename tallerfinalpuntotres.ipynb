{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obteniendo los datos y creando el RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos el conjunto de datos reducido (10 por ciento) proporcionado para la Copa KDD 1999, que contiene casi medio millón de interacciones de red. El archivo se proporciona como un archivo Gzip que descargaremos localmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "f = urllib.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"kddcup.data_10_percent.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos usar este archivo para crear nuestro RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"./kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La transformación del filtro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Esta transformación se puede aplicar a los RDD para mantener elementos justos que satisfagan una determinada condición. Más concretamente, se evalúa una función en cada elemento en el RDD original. El nuevo RDD resultante contendrá solo aquellos elementos que hacen que la función devuelva True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, imaginar que queremos contar cuántas interacciones normales que tenemos en nuestro conjunto de datos. Podemos filtrar nuestro RDD raw_data de la siguiente manera.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_raw_data = raw_data.filter(lambda x: 'normal.' in x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ahora podemos contar cuántos elementos tenemos en el nuevo RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 97278 'normal' interactions\n",
      "Count completed in 5.951 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "normal_count = normal_raw_data.count()\n",
    "tt = time() - t0\n",
    "print \"There are {} 'normal' interactions\".format(normal_count)\n",
    "print \"Count completed in {} seconds\".format(round(tt,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordar de la libreta 1 que tenemos un total de 494021 en nuestro conjunto de datos del 10 por ciento. Aquí podemos ver que 97278 contiene lo normal. palabra de etiqueta  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tener en cuenta que hemos medido el tiempo transcurrido para contar los elementos en el RDD. Hemos hecho esto porque queríamos señalar que los cálculos reales (distribuidos) en Spark tienen lugar cuando ejecutamos * acciones * y no * transformaciones *. En este caso, `count` es la acción que ejecutamos en el RDD. Podemos aplicar tantas transformaciones como queramos en un RDD y no habrá computación hasta que no llamemos a la primera acción que, en este caso, tarda unos segundos en completarse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La transformación del mapa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al utilizar la transformación del mapa en Spark, podemos aplicar una función a cada elemento en nuestro RDD. Las lambdas de Python son especialmente expresivas para este particular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En este caso, queremos leer nuestro archivo de datos como formato CSV. Podemos hacer esto aplicando una función lambda a cada elemento en el RDD de la siguiente manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse completed in 1.715 seconds\n",
      "[u'0',\n",
      " u'tcp',\n",
      " u'http',\n",
      " u'SF',\n",
      " u'181',\n",
      " u'5450',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'1',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'8',\n",
      " u'8',\n",
      " u'0.00',\n",
      " u'0.00',\n",
      " u'0.00',\n",
      " u'0.00',\n",
      " u'1.00',\n",
      " u'0.00',\n",
      " u'0.00',\n",
      " u'9',\n",
      " u'9',\n",
      " u'1.00',\n",
      " u'0.00',\n",
      " u'0.11',\n",
      " u'0.00',\n",
      " u'0.00',\n",
      " u'0.00',\n",
      " u'0.00',\n",
      " u'0.00',\n",
      " u'normal.']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "t0 = time()\n",
    "head_rows = csv_data.take(5)\n",
    "tt = time() - t0\n",
    "print \"Parse completed in {} seconds\".format(round(tt,3))\n",
    "pprint(head_rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente, toda acción ocurre una vez que llamamos a la primera acción Spark (es decir, tome en este caso). ¿Qué pasa si tomamos muchos elementos en lugar de solo los primeros?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse completed in 8.629 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "head_rows = csv_data.take(100000)\n",
    "tt = time() - t0\n",
    "print \"Parse completed in {} seconds\".format(round(tt,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que lleva más tiempo. La función `map` se aplica ahora de forma distribuida a muchos elementos en el RDD, de ahí el tiempo de ejecución más largo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar mapa y funciones predefinidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por supuesto, podemos usar funciones predefinidas con el mapa. Imagine que queremos tener cada elemento en el RDD como un par clave-valor donde la clave es la etiqueta (por ejemplo, normal) y el valor es toda la lista de elementos que representa la fila en el archivo con formato CSV. Podríamos proceder de la siguiente manera.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'normal.',\n",
      " [u'0',\n",
      "  u'tcp',\n",
      "  u'http',\n",
      "  u'SF',\n",
      "  u'181',\n",
      "  u'5450',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'1',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'8',\n",
      "  u'8',\n",
      "  u'0.00',\n",
      "  u'0.00',\n",
      "  u'0.00',\n",
      "  u'0.00',\n",
      "  u'1.00',\n",
      "  u'0.00',\n",
      "  u'0.00',\n",
      "  u'9',\n",
      "  u'9',\n",
      "  u'1.00',\n",
      "  u'0.00',\n",
      "  u'0.11',\n",
      "  u'0.00',\n",
      "  u'0.00',\n",
      "  u'0.00',\n",
      "  u'0.00',\n",
      "  u'0.00',\n",
      "  u'normal.'])\n"
     ]
    }
   ],
   "source": [
    "def parse_interaction(line):\n",
    "    elems = line.split(\",\")\n",
    "    tag = elems[41]\n",
    "    return (tag, elems)\n",
    "\n",
    "key_csv_data = raw_data.map(parse_interaction)\n",
    "head_rows = key_csv_data.take(5)\n",
    "pprint(head_rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La acción `collect`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora hemos utilizado las acciones de contar y tomar. Otra acción básica que debemos aprender es recopilar. Básicamente, tendrá todos los elementos en el RDD en la memoria para que podamos trabajar con ellos. Por esta razón, debe usarse con cuidado, especialmente cuando se trabaja con RDD grandes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ejemplo usando nuestros datos sin procesar.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collected in 17.927 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "all_raw_data = raw_data.collect()\n",
    "tt = time() - t0\n",
    "print \"Data collected in {} seconds\".format(round(tt,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eso tomó más tiempo que cualquier otra acción que usamos antes, por supuesto. Cada nodo del trabajador Spark que tiene un fragmento del RDD tiene que coordinarse para recuperar su parte y luego reducir todo junto.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Como último ejemplo que combina todo lo anterior, queremos recopilar todas las interacciones normales como pares clave-valor.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collected in 12.485 seconds\n",
      "There are 97278 normal interactions\n"
     ]
    }
   ],
   "source": [
    "# get data from file\n",
    "data_file = \"./kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)\n",
    "\n",
    "# parse into key-value pairs\n",
    "key_csv_data = raw_data.map(parse_interaction)\n",
    "\n",
    "# filter normal key interactions\n",
    "normal_key_interactions = key_csv_data.filter(lambda x: x[0] == \"normal.\")\n",
    "\n",
    "# collect all\n",
    "t0 = time()\n",
    "all_normal = normal_key_interactions.collect()\n",
    "tt = time() - t0\n",
    "normal_count = len(all_normal)\n",
    "print \"Data collected in {} seconds\".format(round(tt,3))\n",
    "print \"There are {} 'normal' interactions\".format(normal_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este recuento coincide con el recuento anterior de las interacciones normales. El nuevo procedimiento requiere más tiempo. Esto se debe a que recuperamos todos los datos con collect y luego usamos Python's len en la lista resultante. Antes solo contábamos la cantidad total de elementos en el RDD usando count.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
